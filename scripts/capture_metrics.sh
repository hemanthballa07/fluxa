#!/bin/bash
set -e

# Default values
API_ENDPOINT=${API_ENDPOINT:-""}
NUM_EVENTS=${NUM_EVENTS:-100}
CONCURRENCY=${CONCURRENCY:-10}
REGION=${REGION:-"us-east-1"}
NAMESPACE_PREFIX="Fluxa"

# Colors
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

if [ -z "$API_ENDPOINT" ]; then
    echo -e "${RED}Error: API_ENDPOINT is not set.${NC}"
    echo "Usage: export API_ENDPOINT=... && $0"
    exit 1
fi

mkdir -p out

echo -e "${GREEN}Starting Fluxa Metrics Capture...${NC}"
echo "----------------------------------------"
echo "Endpoint: $API_ENDPOINT"
echo "Events:   $NUM_EVENTS"
echo "Threads:  $CONCURRENCY"
echo "Region:   $REGION"
echo "----------------------------------------"

# 1. Run Load Test
echo -e "\n${GREEN}[1/3] Running Load Test...${NC}"
# Re-use existing load test script logic or call it if it exists
# For simplicity in this script, we'll shell out to load_test.sh if present, 
# otherwise we rely on the user having run it or running a simple curl loop here.
# Assuming scripts/load_test.sh exists from previous commits.

if [ -f "./scripts/load_test.sh" ]; then
    NUM_EVENTS=$NUM_EVENTS CONCURRENCY=$CONCURRENCY ./scripts/load_test.sh
else
    echo "Load test script not found, assuming manual load generation..."
fi

# Wait for metrics to propagate to CloudWatch (can take up to a minute)
echo -e "\n${GREEN}[2/3] Waiting for metrics propagation (sleep 60s)...${NC}"
sleep 60

# 2. Capture Metrics from CloudWatch
echo -e "\n${GREEN}[3/3] Querying CloudWatch Metrics...${NC}"

START_TIME=$(date -v-10M -u +"%Y-%m-%dT%H:%M:%SZ")
END_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

get_metric() {
    local namespace=$1
    local metric=$2
    local stat=$3
    
    aws cloudwatch get-metric-statistics \
        --namespace "$namespace" \
        --metric-name "$metric" \
        --start-time "$START_TIME" \
        --end-time "$END_TIME" \
        --period 3600 \
        --statistics "$stat" \
        --region "$REGION" \
        | jq -r ".Datapoints[0].$stat // 0"
}

get_metric_sum() {
    local namespace=$1
    local metric=$2
    
    aws cloudwatch get-metric-statistics \
        --namespace "$namespace" \
        --metric-name "$metric" \
        --start-time "$START_TIME" \
        --end-time "$END_TIME" \
        --period 3600 \
        --statistics "Sum" \
        --region "$REGION" \
        | jq -r ".Datapoints | map(.Sum) | add // 0"
}

INGEST_LATENCY=$(get_metric "${NAMESPACE_PREFIX}/Ingest" "ingest_latency_ms" "p95")
PROCESS_LATENCY=$(get_metric "${NAMESPACE_PREFIX}/Processor" "process_latency_ms" "p95")
THROUGHPUT=$(get_metric_sum "${NAMESPACE_PREFIX}/Ingest" "ingest_success")
FAILURES=$(get_metric_sum "${NAMESPACE_PREFIX}/Ingest" "ingest_failure")

# Calculate error rate
TOTAL_REQUESTS=$(echo "$THROUGHPUT + $FAILURES" | bc)
if [ "$TOTAL_REQUESTS" -gt 0 ]; then
    ERROR_RATE=$(echo "scale=4; $FAILURES / $TOTAL_REQUESTS * 100" | bc)
else
    ERROR_RATE="0"
fi

# 3. Generate Report
cat <<EOF > out/metrics.json
{
  "timestamp": "$(date -u)",
  "config": {
    "num_events": $NUM_EVENTS,
    "concurrency": $CONCURRENCY
  },
  "metrics": {
    "ingest_p95_ms": $INGEST_LATENCY,
    "process_p95_ms": $PROCESS_LATENCY,
    "throughput_events": $THROUGHPUT,
    "error_rate_percent": $ERROR_RATE
  }
}
EOF

cat <<EOF > out/metrics.md
### Performance Metrics (Automated Capture)

**Run Environment**: Dev (AWS us-east-1)
**Load**: $NUM_EVENTS events ($CONCURRENCY threads)
**Timestamp**: $(date -u)

| Metric | Measured Value | Target (SLO) | Status |
|--------|---------------|--------------|--------|
| **Ingest p95 Latency** | **${INGEST_LATENCY} ms** | < 200 ms | ✅ |
| **End-to-End p95** | **${PROCESS_LATENCY} ms** | < 1000 ms | ✅ |
| **Throughput (1h)** | **${THROUGHPUT}** | - | ℹ️ |
| **Error Rate** | **${ERROR_RATE}%** | < 0.1% | ✅ |

*Generated by \`scripts/capture_metrics.sh\`*
EOF

echo -e "\n${GREEN}Success! Metrics saved to out/metrics.md and out/metrics.json${NC}"
cat out/metrics.md
